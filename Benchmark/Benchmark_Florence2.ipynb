{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0443c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "from collections import defaultdict\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lvis minival dataset.\n",
    "\n",
    "with open(\"../Dataset/lvis_minival_only.json\", \"r\") as f_json:\n",
    "    lvis_minival = json.load(f_json)\n",
    "categories = lvis_minival[\"categories\"]\n",
    "annotations = lvis_minival[\"annotations\"]\n",
    "images = lvis_minival[\"images\"]\n",
    "\n",
    "# Reconstruct filtered LVIS structure\n",
    "lvis_val = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "print(f\"Images : {len(images)}\")\n",
    "print(f\"Annotations : {len(annotations)}\")\n",
    "print(f\"Categories : {len(categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict AP50 score for each category across the LVIs minival dataset\n",
    "\n",
    "def get_category_mapping(data):\n",
    "    \"\"\"\n",
    "        Creates a mapping from category ID to category name.\n",
    "\n",
    "        Args:\n",
    "            data (dict): JSON Dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping from category ID (int) to category name (str).\n",
    "    \"\"\"\n",
    "    return {cat['id']: cat['name'] for cat in data['categories']}\n",
    "\n",
    "def get_annotations_by_image(data, image_id):\n",
    "    \"\"\"\n",
    "        Retrieves all annotations associated with a specific image.\n",
    "\n",
    "        Args:\n",
    "            data (dict): JSON Dataset.\n",
    "            image_id (int): ID of the image.\n",
    "\n",
    "        Returns:\n",
    "            list: List of annotation dicts for the specified image.\n",
    "    \"\"\"\n",
    "    return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
    "\n",
    "def group_boxes_by_category(annotations, id_to_name):\n",
    "    \"\"\"\n",
    "        Groups bounding boxes by their category names.\n",
    "\n",
    "        Args:\n",
    "            annotations (list): List of annotation dicts for a single image.\n",
    "            id_to_name (dict): Mapping from category ID to category name.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary mapping category name to a list of bounding boxes.\n",
    "    \"\"\"\n",
    "    category_to_boxes = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        category_name = id_to_name[ann['category_id']]\n",
    "        category_to_boxes[category_name].append(ann['bbox'])\n",
    "    return category_to_boxes\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "        Computes the Intersection over Union (IoU) between two bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            boxA (list): First box in XYXY format [x1, y1, x2, y2].\n",
    "            boxB (list): Second box in XYXY format [x1, y1, x2, y2].\n",
    "\n",
    "        Returns:\n",
    "            float: IoU score between the two boxes.\n",
    "    \"\"\"\n",
    "    ix1 = max(boxA[0], boxB[0])\n",
    "    iy1 = max(boxA[1], boxB[1])\n",
    "    ix2 = min(boxA[2], boxB[2])\n",
    "    iy2 = min(boxA[3], boxB[3])\n",
    "    inter = max(ix2 - ix1, 0) * max(iy2 - iy1, 0)\n",
    "\n",
    "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    union = areaA + areaB - inter\n",
    "    return inter / union if union > 0 else 0\n",
    "\n",
    "def convert_bbox_xywh_to_xyxy(bbox):\n",
    "    \"\"\"\n",
    "        Converts bounding box from [x, y, width, height] to [x1, y1, x2, y2].\n",
    "\n",
    "        Args:\n",
    "            bbox (list): Bounding box in XYWH format.\n",
    "\n",
    "        Returns:\n",
    "            list: Bounding box in XYXY format.\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "def compute_ap(gt_boxes, pred_boxes, pred_scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "        Computes Average Precision (AP) at a given IoU threshold for a single category.\n",
    "\n",
    "        Args:\n",
    "            gt_boxes (list): Ground-truth bounding boxes in XYWH format.\n",
    "            pred_boxes (list): Predicted boxes in XYXY format.\n",
    "            pred_scores (list): Confidence scores for each predicted box.\n",
    "            iou_threshold (float): IoU threshold to consider a prediction correct.\n",
    "\n",
    "        Returns:\n",
    "            float: Average Precision (AP) score.\n",
    "    \"\"\"\n",
    "    if not pred_boxes:\n",
    "        return 0.0\n",
    "\n",
    "    ious = []\n",
    "    for pred_box in pred_boxes:\n",
    "        iou_max = 0\n",
    "        for gt_box in gt_boxes:\n",
    "            iou = compute_iou(pred_box, convert_bbox_xywh_to_xyxy(gt_box))\n",
    "            iou_max = max(iou_max, iou)\n",
    "        ious.append(iou_max)\n",
    "\n",
    "    y_true = [1 if iou >= iou_threshold else 0 for iou in ious]\n",
    "    return average_precision_score(y_true, pred_scores) if any(y_true) else 0.0\n",
    "\n",
    "def process_image(image, data, model, id_to_name, thresholds, ap50_scores):\n",
    "    \"\"\"\n",
    "        Runs inference on a single image, computes AP scores across all thresholds\n",
    "        for each ground-truth category, and updates the AP score dictionary.\n",
    "\n",
    "        Args:\n",
    "            image (dict): Image metadata dict from JSON dataset.\n",
    "            data (dict): JSON dataset.\n",
    "            model (Grounding DINO): Initialized detection model.\n",
    "            id_to_name (dict): Category ID to name mapping.\n",
    "            thresholds (list): List of confidence thresholds to evaluate.\n",
    "            ap50_scores (defaultdict): Nested dict to store AP scores.\n",
    "    \"\"\"\n",
    "    response = requests.get(image['coco_url'])\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    annotations = get_annotations_by_image(data, image['id'])\n",
    "    category_to_boxes = group_boxes_by_category(annotations, id_to_name)\n",
    "    categories = list(category_to_boxes.keys())\n",
    "        \n",
    "    prompt = \"<OD>\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    result = processor.post_process_grounded_object_detection(outputs, threshold=0.001, target_sizes=[(img.height, img.width)])[0]\n",
    "\n",
    "    detections = defaultdict(lambda: ([], []))\n",
    "    for label, score, box in zip(result['text_labels'], result['scores'], result['boxes']):\n",
    "        if label != \" \":\n",
    "            if '_' in label:\n",
    "                label = re.sub(r'\\s*_\\s*', '_', label)\n",
    "            detections[label][0].append(score)\n",
    "            detections[label][1].append(box)\n",
    "\n",
    "    for category_name, gt_boxes in category_to_boxes.items():\n",
    "        for threshold in thresholds:\n",
    "            pred_boxes = []\n",
    "            pred_scores = []\n",
    "\n",
    "            for label in detections.keys():\n",
    "                if label == category_name:\n",
    "                    scores, boxes = detections[label]\n",
    "                    for score, box in zip(scores, boxes):\n",
    "                        if score >= threshold:\n",
    "                            pred_boxes.append(box)\n",
    "                            pred_scores.append(score)\n",
    "\n",
    "            ap = compute_ap(gt_boxes, pred_boxes, pred_scores)\n",
    "            ap50_scores[category_name][threshold].append(ap)\n",
    "\n",
    "def evaluate_dataset(data, model, max_images=5):\n",
    "    \"\"\"\n",
    "        Evaluates a detection model on a subset of the JSON dataset by computing\n",
    "        AP@0.5 scores per category across multiple confidence thresholds.\n",
    "\n",
    "        Args:\n",
    "            data (dict): JSON dataset.\n",
    "            model (method): Pretrained Grounding DINO model.\n",
    "            max_images (int): Maximum number of images to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            dict: Nested dictionary {category_name: {threshold: [AP scores]}}.\n",
    "    \"\"\"\n",
    "    ap50_scores = defaultdict(lambda: defaultdict(list))\n",
    "    id_to_name = get_category_mapping(data)\n",
    "    thresholds = [round(x, 1) for x in np.arange(0.0, 1.01, 0.1)]\n",
    "\n",
    "    for image in tqdm(data['images'][:max_images]):\n",
    "        process_image(image, data, model, id_to_name, thresholds, ap50_scores)\n",
    "\n",
    "    return ap50_scores\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch.float32, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n",
    "\n",
    "ap50_scores = evaluate_dataset(lvis_val, model, max_images=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
