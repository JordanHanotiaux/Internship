{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPM7TsRDjt4MpuJWCXmkQ2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanHanotiaux/Internship/blob/main/Benchmark_YOLOE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BfXh1EEpjqBB",
        "outputId": "40bede47-ebe2-4fa3-bcbc-eed445744d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.163)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision) (1.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from supervision) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.11/dist-packages (from supervision) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from supervision) (2.32.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision) (1.15.3)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.67.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "!pip install supervision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/GLIPModel/GLIP/resolve/main/mdetr_annotations/final_flickr_separateGT_train.json \\\n",
        "    -P /content/annotations/\n",
        "\n",
        "!wget https://huggingface.co/GLIPModel/GLIP/resolve/main/mdetr_annotations/final_mixed_train_no_coco.json \\\n",
        "    -P /content/annotations/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/datasets/Objects365.yaml \\\n",
        "    -P /content/annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tCT5y3gk0dh",
        "outputId": "fdb68cbb-cf83-4651-8794-1bfe0ea81006"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-09 10:18:49--  https://huggingface.co/GLIPModel/GLIP/resolve/main/mdetr_annotations/final_flickr_separateGT_train.json\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.102.58, 3.165.102.6, 3.165.102.22, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.102.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/13/50/135006ff86825f748a869e2da849789ef2c4adf60ca9f4b959caa837f55c5082/00805253492afd77c066b7132612f8c5fb6a4d0f8bff6d593cbfec23e1aac173?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27final_flickr_separateGT_train.json%3B+filename%3D%22final_flickr_separateGT_train.json%22%3B&response-content-type=application%2Fjson&Expires=1752059929&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjA1OTkyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMy81MC8xMzUwMDZmZjg2ODI1Zjc0OGE4NjllMmRhODQ5Nzg5ZWYyYzRhZGY2MGNhOWY0Yjk1OWNhYTgzN2Y1NWM1MDgyLzAwODA1MjUzNDkyYWZkNzdjMDY2YjcxMzI2MTJmOGM1ZmI2YTRkMGY4YmZmNmQ1OTNjYmZlYzIzZTFhYWMxNzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=q%7EwKWYfaPGjlqkASdtQKkq%7EBZsOahHNdof3iOiZXp%7Eamy9HJj8Z9hPhVwojyCArvpHZJivFgRMAN%7EKkvl7s-GE7dy7ZmEyot0xxSLKqyfgFjH60bD-jvgywOkYPDWuBaICnpcs8NFr8b3w8mHpr6G%7EIpTRXLrkuuFq9iTT5v-eV6kp6LyC3vu4pxv4LR-XdtpRO4qFCYwSs5JPC36yHX05QKvLUOhBHTEicf3TBjE-rjjloD%7El0Hd9Q4HGLjc9GSH5oMxPwISbEEQurxEkgOKWL4H7BELCoIeCPpIvD320tN5%7E1m1wCWjQPLCPi-OcaWZ9PGgD6Ga8WiApTd%7EKTpCQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-07-09 10:18:49--  https://cdn-lfs.hf.co/repos/13/50/135006ff86825f748a869e2da849789ef2c4adf60ca9f4b959caa837f55c5082/00805253492afd77c066b7132612f8c5fb6a4d0f8bff6d593cbfec23e1aac173?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27final_flickr_separateGT_train.json%3B+filename%3D%22final_flickr_separateGT_train.json%22%3B&response-content-type=application%2Fjson&Expires=1752059929&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjA1OTkyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMy81MC8xMzUwMDZmZjg2ODI1Zjc0OGE4NjllMmRhODQ5Nzg5ZWYyYzRhZGY2MGNhOWY0Yjk1OWNhYTgzN2Y1NWM1MDgyLzAwODA1MjUzNDkyYWZkNzdjMDY2YjcxMzI2MTJmOGM1ZmI2YTRkMGY4YmZmNmQ1OTNjYmZlYzIzZTFhYWMxNzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=q%7EwKWYfaPGjlqkASdtQKkq%7EBZsOahHNdof3iOiZXp%7Eamy9HJj8Z9hPhVwojyCArvpHZJivFgRMAN%7EKkvl7s-GE7dy7ZmEyot0xxSLKqyfgFjH60bD-jvgywOkYPDWuBaICnpcs8NFr8b3w8mHpr6G%7EIpTRXLrkuuFq9iTT5v-eV6kp6LyC3vu4pxv4LR-XdtpRO4qFCYwSs5JPC36yHX05QKvLUOhBHTEicf3TBjE-rjjloD%7El0Hd9Q4HGLjc9GSH5oMxPwISbEEQurxEkgOKWL4H7BELCoIeCPpIvD320tN5%7E1m1wCWjQPLCPi-OcaWZ9PGgD6Ga8WiApTd%7EKTpCQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.37, 18.155.68.34, 18.155.68.87, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144527532 (138M) [application/json]\n",
            "Saving to: ‘/content/annotations/final_flickr_separateGT_train.json’\n",
            "\n",
            "final_flickr_separa 100%[===================>] 137.83M   429MB/s    in 0.3s    \n",
            "\n",
            "2025-07-09 10:18:50 (429 MB/s) - ‘/content/annotations/final_flickr_separateGT_train.json’ saved [144527532/144527532]\n",
            "\n",
            "--2025-07-09 10:18:50--  https://huggingface.co/GLIPModel/GLIP/resolve/main/mdetr_annotations/final_mixed_train_no_coco.json\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.102.6, 3.165.102.128, 3.165.102.22, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.102.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/13/50/135006ff86825f748a869e2da849789ef2c4adf60ca9f4b959caa837f55c5082/3097d08cdbe7469bdc07a7d71bfe4cc8ec230d7b5488c3609d93acdfb3a8a3a2?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27final_mixed_train_no_coco.json%3B+filename%3D%22final_mixed_train_no_coco.json%22%3B&response-content-type=application%2Fjson&Expires=1752059930&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjA1OTkzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMy81MC8xMzUwMDZmZjg2ODI1Zjc0OGE4NjllMmRhODQ5Nzg5ZWYyYzRhZGY2MGNhOWY0Yjk1OWNhYTgzN2Y1NWM1MDgyLzMwOTdkMDhjZGJlNzQ2OWJkYzA3YTdkNzFiZmU0Y2M4ZWMyMzBkN2I1NDg4YzM2MDlkOTNhY2RmYjNhOGEzYTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=ru-itvNXAC-gbW7W1AR7eJtDEqnE%7Ei2d76g9woCsV0j%7EIaodgm1J4N8EmEpezEPBP9RIg%7EHUAegdfkt07Qq6hBJ%7EuaN%7Epr1UKu6lEcMyCdbzYDRmuz2OVef5maYEg8EIGH7IufUarohM5ftjez%7E3NFqhL6P7HS7uHr8Bqn28BRbD0aOw-pLWrzC9YhY1kmfM7ZyCzNEWU1aZgFb904h5ry3lqFalmgjq8f7ydKuudEzdLP1VA05oHfF08TlK-aW8QQZfFqV3dVFRjKKrsLHFYpNUbcXZlT8-ygmgt38wBmaICwNd7Y9L8KnQs9PMsMmnkcc3X6FzPggtmelWowe%7Emg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-07-09 10:18:50--  https://cdn-lfs.hf.co/repos/13/50/135006ff86825f748a869e2da849789ef2c4adf60ca9f4b959caa837f55c5082/3097d08cdbe7469bdc07a7d71bfe4cc8ec230d7b5488c3609d93acdfb3a8a3a2?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27final_mixed_train_no_coco.json%3B+filename%3D%22final_mixed_train_no_coco.json%22%3B&response-content-type=application%2Fjson&Expires=1752059930&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjA1OTkzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMy81MC8xMzUwMDZmZjg2ODI1Zjc0OGE4NjllMmRhODQ5Nzg5ZWYyYzRhZGY2MGNhOWY0Yjk1OWNhYTgzN2Y1NWM1MDgyLzMwOTdkMDhjZGJlNzQ2OWJkYzA3YTdkNzFiZmU0Y2M4ZWMyMzBkN2I1NDg4YzM2MDlkOTNhY2RmYjNhOGEzYTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=ru-itvNXAC-gbW7W1AR7eJtDEqnE%7Ei2d76g9woCsV0j%7EIaodgm1J4N8EmEpezEPBP9RIg%7EHUAegdfkt07Qq6hBJ%7EuaN%7Epr1UKu6lEcMyCdbzYDRmuz2OVef5maYEg8EIGH7IufUarohM5ftjez%7E3NFqhL6P7HS7uHr8Bqn28BRbD0aOw-pLWrzC9YhY1kmfM7ZyCzNEWU1aZgFb904h5ry3lqFalmgjq8f7ydKuudEzdLP1VA05oHfF08TlK-aW8QQZfFqV3dVFRjKKrsLHFYpNUbcXZlT8-ygmgt38wBmaICwNd7Y9L8KnQs9PMsMmnkcc3X6FzPggtmelWowe%7Emg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.85, 18.155.68.87, 18.155.68.37, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 835055416 (796M) [application/json]\n",
            "Saving to: ‘/content/annotations/final_mixed_train_no_coco.json’\n",
            "\n",
            "  final_mixed_train  54%[=========>          ] 437.58M   273MB/s               ^C\n",
            "--2025-07-09 10:18:52--  https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/datasets/Objects365.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9355 (9.1K) [text/plain]\n",
            "Saving to: ‘/content/annotations/Objects365.yaml’\n",
            "\n",
            "Objects365.yaml     100%[===================>]   9.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-09 10:18:52 (112 MB/s) - ‘/content/annotations/Objects365.yaml’ saved [9355/9355]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import re\n",
        "import yaml\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from google.colab import drive\n",
        "from ultralytics import YOLOE\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLOE\n",
        "from sklearn.metrics import average_precision_score\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "Z4XrGTnCjypU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Lvis minival dataset.\n",
        "\n",
        "!rm -rf lvis_minival_only.json\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(\"/content/drive/MyDrive/minival.txt\", \"r\") as f:\n",
        "    minival_ids = set(int(line.strip().split('/')[-1].split('.')[0]) for line in f)\n",
        "\n",
        "images = []\n",
        "annotations = []\n",
        "categories = []\n",
        "category_ids_in_minival = set()\n",
        "\n",
        "# Extract and filter LVIS data from the ZIP archive\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/lvis_v1_val.json.zip\", \"r\") as zf:\n",
        "    with zf.open(\"lvis_v1_val.json\", \"r\") as f:\n",
        "        lvis_data = json.load(f)\n",
        "\n",
        "    for img in lvis_data[\"images\"]:\n",
        "        if img[\"id\"] in minival_ids:\n",
        "            images.append(img)\n",
        "\n",
        "    for ann in lvis_data[\"annotations\"]:\n",
        "        if ann[\"image_id\"] in minival_ids:\n",
        "            annotations.append(ann)\n",
        "            category_ids_in_minival.add(ann['category_id'])\n",
        "\n",
        "    for cat in lvis_data[\"categories\"]:\n",
        "        if cat['id'] in category_ids_in_minival:\n",
        "            categories.append(cat)\n",
        "\n",
        "# Reconstruct filtered LVIS structure\n",
        "lvis_data = {\n",
        "    \"images\": images,\n",
        "    \"annotations\": annotations,\n",
        "    \"categories\": categories\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"lvis_minival_only.json\", \"w\") as f:\n",
        "    json.dump(lvis_data, f)\n",
        "\n",
        "print(f\"Images : {len(images)}\")\n",
        "print(f\"Annotations : {len(annotations)}\")\n",
        "print(f\"Categories : {len(categories)}\")"
      ],
      "metadata": {
        "id": "5gn1t5egj1Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create categories's set of Lvis dataset\n",
        "\n",
        "def clean_category(name):\n",
        "    # Delete pattern as _(xxx)\n",
        "    name = re.sub(r'_\\([^)]*\\)', '', name)\n",
        "    # Replace remaining underscores by spaces\n",
        "    name = re.sub(r'_', ' ', name)\n",
        "    return name.strip()\n",
        "\n",
        "lvis_categories = set()\n",
        "for cat in lvis_val['categories']:\n",
        "    cleaned = clean_category(cat['name'])\n",
        "    lvis_categories.add(cleaned)\n",
        "\n",
        "print(\"Nombre de catégories (LVIS dataset):\", len(lvis_categories))"
      ],
      "metadata": {
        "id": "O5TFfrdacUSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Object365 dataset\n",
        "\n",
        "with open('/content/annotations/Objects365.yaml') as f:\n",
        "    O365_train = yaml.safe_load(f)\n",
        "\n",
        "object365_categories_processed = set()\n",
        "for v in O365_train['names'].values():\n",
        "    for word in v.split('/'):\n",
        "        object365_categories_processed.add(word.lower().strip())\n",
        "\n",
        "print(\"Nombre de catégories (Objects365 dataset):\", len(object365_categories_processed))\n",
        "\n",
        "# Calcul des catégories communes et uniques\n",
        "seen_categories = lvis_categories.intersection(object365_categories_processed)\n",
        "print(\"Nombre de catégories communes (LVIS-Object365) :\", len(seen_categories))\n",
        "\n",
        "unseen_categories = lvis_categories.difference(object365_categories_processed)\n",
        "print(\"Nombre de catégories non communes (LVIS-Object365) :\", len(unseen_categories))"
      ],
      "metadata": {
        "id": "xTVm_tyJk709"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import GoldG dataset\n",
        "\n",
        "def import_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "flickr_train = import_json_file('/content/annotations/final_flickr_separateGT_train.json')\n",
        "gqa_train = import_json_file('/content/annotations/final_mixed_train_no_coco.json')\n",
        "\n",
        "# Info dataset\n",
        "print(\" Flickr\")\n",
        "print(\" - Nombre d'images :\", len(flickr_train['images']))\n",
        "print(\" - Nombre d'annotations :\", len(flickr_train['annotations']))\n",
        "\n",
        "print(\"\\n GQA\")\n",
        "print(\" - Nombre d'images :\", len(gqa_train['images']))\n",
        "print(\" - Nombre d'annotations :\", len(gqa_train['annotations']))\n",
        "\n",
        "# Extract annotated phrases (grounding categories) from captions\n",
        "def extract_grounding_phrases(dataset):\n",
        "    categories = set()\n",
        "    for data in dataset:\n",
        "      annotations = data['annotations']\n",
        "      ann_idx = 0\n",
        "      for img in data['images']:\n",
        "          caption = img['caption'].lower()\n",
        "          image_id = img['id']\n",
        "          while ann_idx < len(annotations) and annotations[ann_idx]['image_id'] == image_id:\n",
        "              for interval in annotations[ann_idx]['tokens_positive']:\n",
        "                  phrase = caption[interval[0]:interval[1]]\n",
        "                  categories.add(phrase.strip())\n",
        "              ann_idx += 1\n",
        "          ann_idx += 1\n",
        "    return categories\n",
        "\n",
        "grounding_categories = extract_grounding_phrases([flickr_train, gqa_train])\n",
        "\n",
        "print(\"\\n Résumé\")\n",
        "print(f\" - Nombre de catégories : {len(grounding_categories)}\")\n",
        "print(f\" - Exemples : {list(grounding_categories)[:5]}\")\n",
        "\n",
        "# Match unseen categories with annotated phrases (grounding categories)\n",
        "example = 0\n",
        "for category in unseen_categories.copy():\n",
        "    for phrase in grounding_categories:\n",
        "      if all(word in phrase.split() for word in category.split()):\n",
        "        if example < 5:\n",
        "          print(\"Catégorie trouvée :\", category)\n",
        "          print(\"Phrase trouvée :\", phrase)\n",
        "          example += 1\n",
        "        unseen_categories.discard(category)\n",
        "        seen_categories.add(category)\n",
        "        break\n",
        "\n",
        "print(\"Nombre de catégories communes(LVIS-Object 365-GoldG dataset):\", len(seen_categories))\n",
        "print(\"Nombre de catégories non communes (LVIS-Object 365-GoldG dataset):\", len(unseen_categories))"
      ],
      "metadata": {
        "id": "3tXvzxXDlsMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict AP50 score for each category across the LVIs minival dataset\n",
        "\n",
        "def get_category_mapping(data):\n",
        "  \"\"\"\n",
        "    Creates a mapping from category ID to category name.\n",
        "\n",
        "    Args:\n",
        "        data (dict): JSON Dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping from category ID (int) to category name (str).\n",
        "    \"\"\"\n",
        "    return {cat['id']: cat['name'] for cat in data['categories']}\n",
        "\n",
        "def get_annotations_by_image(data, image_id):\n",
        "  \"\"\"\n",
        "    Retrieves all annotations associated with a specific image.\n",
        "\n",
        "    Args:\n",
        "        data (dict): JSON Dataset.\n",
        "        image_id (int): ID of the image.\n",
        "\n",
        "    Returns:\n",
        "        list: List of annotation dicts for the specified image.\n",
        "    \"\"\"\n",
        "    return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
        "\n",
        "def group_boxes_by_category(annotations, id_to_name):\n",
        "  \"\"\"\n",
        "    Groups bounding boxes by their category names.\n",
        "\n",
        "    Args:\n",
        "        annotations (list): List of annotation dicts for a single image.\n",
        "        id_to_name (dict): Mapping from category ID to category name.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping category name to a list of bounding boxes.\n",
        "    \"\"\"\n",
        "    category_to_boxes = defaultdict(list)\n",
        "    for ann in annotations:\n",
        "        category_name = id_to_name[ann['category_id']]\n",
        "        category_to_boxes[category_name].append(ann['bbox'])\n",
        "    return category_to_boxes\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "  \"\"\"\n",
        "    Computes the Intersection over Union (IoU) between two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        boxA (list): First box in XYXY format [x1, y1, x2, y2].\n",
        "        boxB (list): Second box in XYXY format [x1, y1, x2, y2].\n",
        "\n",
        "    Returns:\n",
        "        float: IoU score between the two boxes.\n",
        "    \"\"\"\n",
        "    ix1 = max(boxA[0], boxB[0])\n",
        "    iy1 = max(boxA[1], boxB[1])\n",
        "    ix2 = min(boxA[2], boxB[2])\n",
        "    iy2 = min(boxA[3], boxB[3])\n",
        "    inter = max(ix2 - ix1, 0) * max(iy2 - iy1, 0)\n",
        "\n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    union = areaA + areaB - inter\n",
        "    return inter / union if union > 0 else 0\n",
        "\n",
        "def convert_bbox_xywh_to_xyxy(bbox):\n",
        "   \"\"\"\n",
        "    Converts bounding box from [x, y, width, height] to [x1, y1, x2, y2].\n",
        "\n",
        "    Args:\n",
        "        bbox (list): Bounding box in XYWH format.\n",
        "\n",
        "    Returns:\n",
        "        list: Bounding box in XYXY format.\n",
        "    \"\"\"\n",
        "    x, y, w, h = bbox\n",
        "    return [x, y, x + w, y + h]\n",
        "\n",
        "def compute_ap(gt_boxes, pred_boxes, pred_scores, iou_threshold=0.5):\n",
        "  \"\"\"\n",
        "    Computes Average Precision (AP) at a given IoU threshold for a single category.\n",
        "\n",
        "    Args:\n",
        "        gt_boxes (list): Ground-truth bounding boxes in XYWH format.\n",
        "        pred_boxes (list): Predicted boxes in XYXY format.\n",
        "        pred_scores (list): Confidence scores for each predicted box.\n",
        "        iou_threshold (float): IoU threshold to consider a prediction correct.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Precision (AP) score.\n",
        "    \"\"\"\n",
        "    if not pred_boxes:\n",
        "        return 0.0\n",
        "\n",
        "    ious = []\n",
        "    for pred_box in pred_boxes:\n",
        "        iou_max = 0\n",
        "        for gt_box in gt_boxes:\n",
        "            iou = compute_iou(pred_box, convert_bbox_xywh_to_xyxy(gt_box))\n",
        "            iou_max = max(iou_max, iou)\n",
        "        ious.append(iou_max)\n",
        "\n",
        "    y_true = [1 if iou >= iou_threshold else 0 for iou in ious]\n",
        "    return average_precision_score(y_true, pred_scores) if any(y_true) else 0.0\n",
        "\n",
        "def process_image(image, data, model, id_to_name, thresholds, ap50_scores):\n",
        "  \"\"\"\n",
        "    Runs inference on a single image, computes AP scores across all thresholds\n",
        "    for each ground-truth category, and updates the AP score dictionary.\n",
        "\n",
        "    Args:\n",
        "        image (dict): Image metadata dict from JSON dataset.\n",
        "        data (dict): JSON dataset.\n",
        "        model (YOLOE): Initialized detection model.\n",
        "        id_to_name (dict): Category ID to name mapping.\n",
        "        thresholds (list): List of confidence thresholds to evaluate.\n",
        "        ap50_scores (defaultdict): Nested dict to store AP scores.\n",
        "    \"\"\"\n",
        "    response = requests.get(image['coco_url'])\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "    annotations = get_annotations_by_image(data, image['id'])\n",
        "    category_to_boxes = group_boxes_by_category(annotations, id_to_name)\n",
        "    categories = list(category_to_boxes.keys())\n",
        "\n",
        "    model.set_classes(categories, model.get_text_pe(categories))\n",
        "    result = model.predict(img, conf=0.001, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "    for category_name, gt_boxes in category_to_boxes.items():\n",
        "        for threshold in thresholds:\n",
        "            pred_boxes = []\n",
        "            pred_scores = []\n",
        "\n",
        "            for label, score, box in zip(detections['class_name'], detections.confidence, detections.xyxy):\n",
        "                if label == category_name and score >= threshold:\n",
        "                    pred_boxes.append(box)\n",
        "                    pred_scores.append(score)\n",
        "\n",
        "            ap = compute_ap(gt_boxes, pred_boxes, pred_scores)\n",
        "            ap50_scores[category_name][threshold].append(ap)\n",
        "\n",
        "def evaluate_dataset(data, model_path, max_images=5):\n",
        "  \"\"\"\n",
        "    Evaluates a detection model on a subset of the JSON dataset by computing\n",
        "    AP@0.5 scores per category across multiple confidence thresholds.\n",
        "\n",
        "    Args:\n",
        "        data (dict): JSON dataset.\n",
        "        model_path (str): Path to the pretrained YOLOE model.\n",
        "        max_images (int): Maximum number of images to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        dict: Nested dictionary {category_name: {threshold: [AP scores]}}.\n",
        "    \"\"\"\n",
        "    ap50_scores = defaultdict(lambda: defaultdict(list))\n",
        "    id_to_name = get_category_mapping(data)\n",
        "    thresholds = [round(x, 1) for x in np.arange(0.0, 1.01, 0.1)]\n",
        "\n",
        "    for image in tqdm(data['images'][:max_images]):\n",
        "        model = YOLOE(model_path)\n",
        "        # model.to(\"cuda\")\n",
        "        process_image(image, data, model, id_to_name, thresholds, ap50_scores)\n",
        "\n",
        "    return ap50_scores\n",
        "\n",
        "ap50_scores = evaluate_dataset(lvis_val, \"pretrain/yoloe-v8l-seg.pt\", max_images=5)"
      ],
      "metadata": {
        "id": "cNC5Gyfklq8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the mean Average Precision (mAP@0.5) across all categories for each confidence threshold,\n",
        "# then plots the mAP@0.5 curve.\n",
        "\n",
        "mAP50_per_threshold = {}\n",
        "for threshold in thresholds:\n",
        "    all_aps_at_threshold = []\n",
        "    for category_name in ap50_scores:\n",
        "        if ap50_scores[category_name][threshold]:\n",
        "            all_aps_at_threshold.extend(ap50_scores[category_name][threshold])\n",
        "    if all_aps_at_threshold:\n",
        "        mAP50_per_threshold[threshold] = np.mean(all_aps_at_threshold)\n",
        "    else:\n",
        "        mAP50_per_threshold[threshold] = 0.0\n",
        "\n",
        "thresholds_sorted = sorted(mAP50_per_threshold.keys())\n",
        "mAP50_values = [mAP50_per_threshold[t] for t in thresholds_sorted]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds_sorted, mAP50_values, marker='o')\n",
        "plt.xlabel('Confidence Threshold')\n",
        "plt.ylabel('mAP@0.5')\n",
        "plt.title('mAP@0.5 vs. Confidence Threshold')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"mAP@0.5 per Threshold:\")\n",
        "for threshold, mAP50 in mAP50_per_threshold.items():\n",
        "    print(f\"Threshold: {threshold:.1f}, mAP@0.5: {mAP50:.4f}\")"
      ],
      "metadata": {
        "id": "kuSAtrm2KbFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rare_categories = set()\n",
        "common_categories = set()\n",
        "frequent_categories = set()\n",
        "\n",
        "for cat in lvis_val['categories']:\n",
        "  freq = cat['frequency']\n",
        "  if freq == 'r':\n",
        "    rare_categories.add(cat['name'])\n",
        "  elif freq == 'c':\n",
        "    common_categories.add(cat['name'])\n",
        "  elif freq == 'f':\n",
        "    frequent_categories.add(cat['name'])\n",
        "\n",
        "print(f\"Catégories rares: {len(rare_categories)}\")\n",
        "print(f\"Catégories communes: {len(common_categories)}\")\n",
        "print(f\"Catégories fréquentes: {len(frequent_categories)}\")"
      ],
      "metadata": {
        "id": "yE2wXfYAOi8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_ap50_scores_by_frequency(ap50_scores, frequency_dict):\n",
        "    \"\"\"\n",
        "    Splits AP50 scores into separate dictionaries based on category frequency.\n",
        "\n",
        "    Args:\n",
        "        ap50_scores (dict): Nested dict {category_name: {threshold: [ap_scores]}}.\n",
        "        frequency_dict (dict): Dict with keys 'rare', 'common', 'frequent', each mapping to a set of category names.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of defaultdicts: (rare_scores, common_scores, frequent_scores)\n",
        "    \"\"\"\n",
        "    rare_scores = defaultdict(lambda: defaultdict(list))\n",
        "    common_scores = defaultdict(lambda: defaultdict(list))\n",
        "    frequent_scores = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for category, thresholds_data in ap50_scores.items():\n",
        "        if category in frequency_dict['rare']:\n",
        "            target = rare_scores\n",
        "        elif category in frequency_dict['common']:\n",
        "            target = common_scores\n",
        "        elif category in frequency_dict['frequent']:\n",
        "            target = frequent_scores\n",
        "        else:\n",
        "            continue  # skip categories not in any group\n",
        "\n",
        "        for threshold, aps in thresholds_data.items():\n",
        "            target[category][threshold].extend(aps)\n",
        "\n",
        "    return rare_scores, common_scores, frequent_scores\n",
        "\n",
        "\n",
        "def compute_mAP50_per_threshold(scores_by_category, thresholds):\n",
        "    \"\"\"\n",
        "    Computes mAP@0.5 per threshold across multiple categories.\n",
        "\n",
        "    Args:\n",
        "        scores_by_category (dict): Nested dict {category_name: {threshold: [ap_scores]}}.\n",
        "        thresholds (list): List of thresholds to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping from threshold to averaged mAP@0.5 score.\n",
        "    \"\"\"\n",
        "    mAP50_per_threshold = {}\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        all_scores = []\n",
        "        for category_scores in scores_by_category.values():\n",
        "            all_scores.extend(category_scores.get(threshold, []))\n",
        "\n",
        "        mAP50_per_threshold[threshold] = (\n",
        "            np.mean(all_scores) if all_scores else 0.0\n",
        "        )\n",
        "\n",
        "    return mAP50_per_threshold\n",
        "\n",
        "\n",
        "def plot_mAP_curves(mAP50_dicts, thresholds, labels):\n",
        "    \"\"\"\n",
        "    Plots mAP@0.5 curves for different category frequency groups.\n",
        "\n",
        "    Args:\n",
        "        mAP50_dicts (list of dict): List of threshold->mAP50 dicts.\n",
        "        thresholds (list): List of thresholds used.\n",
        "        labels (list): Corresponding labels for each curve.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for mAP50, label in zip(mAP50_dicts, labels):\n",
        "        sorted_thresh = sorted(mAP50.keys())\n",
        "        sorted_mAP = [mAP50[t] for t in sorted_thresh]\n",
        "        plt.plot(sorted_thresh, sorted_mAP, marker='o', label=label)\n",
        "\n",
        "    plt.xlabel('Confidence Threshold')\n",
        "    plt.ylabel('mAP@0.5')\n",
        "    plt.title('mAP@0.5 vs. Confidence Threshold per Category Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_mAP50(mAP50_per_threshold, label):\n",
        "    \"\"\"\n",
        "    Prints the mAP@0.5 for each threshold.\n",
        "\n",
        "    Args:\n",
        "        mAP50_per_threshold (dict): Mapping threshold -> mAP50.\n",
        "        label (str): Label describing the category group.\n",
        "    \"\"\"\n",
        "    print(f\"\\nmAP@0.5 per Threshold for {label} Categories:\")\n",
        "    for threshold, score in mAP50_per_threshold.items():\n",
        "        print(f\"Threshold: {threshold:.1f}, mAP@0.5: {score:.4f}\")\n",
        "\n",
        "category_frequencies = {\n",
        "    'rare': rare_categories,\n",
        "    'common': common_categories,\n",
        "    'frequent': frequent_categories\n",
        "}\n",
        "\n",
        "rare_scores, common_scores, frequent_scores = split_ap50_scores_by_frequency(\n",
        "    ap50_scores, category_frequencies\n",
        ")\n",
        "\n",
        "rare_mAP50 = compute_mAP50_per_threshold(rare_scores, thresholds)\n",
        "common_mAP50 = compute_mAP50_per_threshold(common_scores, thresholds)\n",
        "frequent_mAP50 = compute_mAP50_per_threshold(frequent_scores, thresholds)\n",
        "\n",
        "plot_mAP_curves(\n",
        "    [rare_mAP50, common_mAP50, frequent_mAP50],\n",
        "    thresholds,\n",
        "    labels=['Rare', 'Common', 'Frequent']\n",
        ")\n",
        "\n",
        "print_mAP50(rare_mAP50, 'Rare')\n",
        "print_mAP50(common_mAP50, 'Common')\n",
        "print_mAP50(frequent_mAP50, 'Frequent')\n",
        "\n"
      ],
      "metadata": {
        "id": "EyJ_zUhiPNg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_categories_by_object_size(lvis_data, id_to_name, small_thresh=32*32, large_thresh=96*96):\n",
        "    \"\"\"\n",
        "    Categorizes object categories into small, normal, and large based on average bounding box area.\n",
        "\n",
        "    Args:\n",
        "        lvis_data (dict): LVIS annotations data.\n",
        "        id_to_name (dict): Mapping from category ID to name.\n",
        "        small_thresh (int): Area threshold under which objects are considered small.\n",
        "        large_thresh (int): Area threshold above which objects are considered large.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with keys 'small', 'normal', 'large' each containing a set of category names.\n",
        "    \"\"\"\n",
        "    small, normal, large = set(), set(), set()\n",
        "\n",
        "    for ann in lvis_data['annotations']:\n",
        "        area = ann['bbox'][2] * ann['bbox'][3]  # width * height\n",
        "        category_name = id_to_name[ann['category_id']]\n",
        "        if area < small_thresh:\n",
        "            small.add(category_name)\n",
        "        elif area > large_thresh:\n",
        "            large.add(category_name)\n",
        "        else:\n",
        "            normal.add(category_name)\n",
        "\n",
        "    # Ensure mutual exclusivity\n",
        "    normal -= small | large\n",
        "    small -= normal | large\n",
        "    large -= small | normal\n",
        "\n",
        "    return {'small': small, 'normal': normal, 'large': large}\n",
        "\n",
        "\n",
        "def split_ap50_scores_by_object_size(ap50_scores, size_categories):\n",
        "    \"\"\"\n",
        "    Splits AP50 scores by object size categories.\n",
        "\n",
        "    Args:\n",
        "        ap50_scores (dict): Nested dict {category_name: {threshold: [ap_scores]}}.\n",
        "        size_categories (dict): Dict with keys 'small', 'normal', 'large', each a set of category names.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of defaultdicts: (small_scores, normal_scores, large_scores)\n",
        "    \"\"\"\n",
        "    small_scores = defaultdict(lambda: defaultdict(list))\n",
        "    normal_scores = defaultdict(lambda: defaultdict(list))\n",
        "    large_scores = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for category, thresholds_data in ap50_scores.items():\n",
        "        if category in size_categories['small']:\n",
        "            target = small_scores\n",
        "        elif category in size_categories['normal']:\n",
        "            target = normal_scores\n",
        "        elif category in size_categories['large']:\n",
        "            target = large_scores\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        for threshold, aps in thresholds_data.items():\n",
        "            target[category][threshold].extend(aps)\n",
        "\n",
        "    return small_scores, normal_scores, large_scores\n",
        "\n",
        "# Step 1: Categorize by object size\n",
        "object_size_categories = categorize_categories_by_object_size(lvis_val, id_to_name)\n",
        "\n",
        "# Step 2: Split AP50 scores by size category\n",
        "small_scores, normal_scores, large_scores = split_ap50_scores_by_object_size(ap50_scores, object_size_categories)\n",
        "\n",
        "# Step 3: Compute mAP@0.5 curves\n",
        "small_mAP50 = compute_mAP50_per_threshold(small_scores, thresholds)\n",
        "normal_mAP50 = compute_mAP50_per_threshold(normal_scores, thresholds)\n",
        "large_mAP50 = compute_mAP50_per_threshold(large_scores, thresholds)\n",
        "\n",
        "# Step 4: Plot\n",
        "plot_mAP_curves(\n",
        "    [small_mAP50, normal_mAP50, large_mAP50],\n",
        "    thresholds,\n",
        "    labels=['Small Objects', 'Normal Objects', 'Large Objects']\n",
        ")\n",
        "\n",
        "# Step 5: Print results\n",
        "print_mAP50(small_mAP50, 'Small Objects')\n",
        "print_mAP50(normal_mAP50, 'Normal Objects')\n",
        "print_mAP50(large_mAP50, 'Large Objects')\n",
        "\n"
      ],
      "metadata": {
        "id": "EJW8dJTKPydo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_ap50_scores_by_seen_status(ap50_scores, seen_categories, unseen_categories, clean_fn):\n",
        "    \"\"\"\n",
        "    Splits AP50 scores into seen and unseen categories based on cleaned names.\n",
        "\n",
        "    Args:\n",
        "        ap50_scores (dict): Nested dict {category_name: {threshold: [ap_scores]}}.\n",
        "        seen_categories (set): Set of cleaned names of seen categories.\n",
        "        unseen_categories (set): Set of cleaned names of unseen categories.\n",
        "        clean_fn (function): Function to clean/normalize category names.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of defaultdicts: (seen_scores, unseen_scores)\n",
        "    \"\"\"\n",
        "    seen_scores = defaultdict(lambda: defaultdict(list))\n",
        "    unseen_scores = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for category, thresholds_data in ap50_scores.items():\n",
        "        clean_name = clean_fn(category)\n",
        "        if clean_name in unseen_categories:\n",
        "            target = unseen_scores\n",
        "        elif clean_name in seen_categories:\n",
        "            target = seen_scores\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        for threshold, aps in thresholds_data.items():\n",
        "            target[category][threshold].extend(aps)\n",
        "\n",
        "    return seen_scores, unseen_scores\n",
        "\n",
        "# Step 1: Split scores by seen/unseen\n",
        "seen_scores, unseen_scores = split_ap50_scores_by_seen_status(\n",
        "    ap50_scores, seen_categories, unseen_categories, clean_category\n",
        ")\n",
        "\n",
        "# Step 2: Compute mAP curves\n",
        "seen_mAP50 = compute_mAP50_per_threshold(seen_scores, thresholds)\n",
        "unseen_mAP50 = compute_mAP50_per_threshold(unseen_scores, thresholds)\n",
        "\n",
        "# Step 3: Plot\n",
        "plot_mAP_curves(\n",
        "    [unseen_mAP50, seen_mAP50],\n",
        "    thresholds,\n",
        "    labels=['Unseen Objects', 'Seen Objects']\n",
        ")\n",
        "\n",
        "# Step 4: Print\n",
        "print_mAP50(seen_mAP50, 'Seen Objects')\n",
        "print_mAP50(unseen_mAP50, 'Unseen Objects')\n"
      ],
      "metadata": {
        "id": "7c4HFmQkcov6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}